# -*- coding: utf-8 -*-
"""Feb27-Multi-threading_processing_async.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qPdbGR_vQVHQFy7bfRu4vTMuyySXXJbr

# MultiThreading
"""

# Simple thread
import threading
import time

def print_number() -> None:
    for i in range(5):
        print(f"Thread: {i}")
        time.sleep(1)

Thread=threading.Thread(target=print_number)
Thread.start()
print("Main thread is running...")

Thread.join()
print("Thread Execution finished")

# Multiple Threads

def worker(task_id: int) -> None:
    """Worker function that prints task ID."""
    print(f"Task {task_id} started")
    time.sleep(1)
    print(f"Task {task_id} completed")

# Creating multiple threads
threads = []
for i in range(5):
    thread = threading.Thread(target=worker, args=(i,))
    threads.append(thread)
    thread.start()

for thread in threads:
    thread.join()

print("All tasks completed.")

# Using ThreadPool Executor

from concurrent.futures import ThreadPoolExecutor

def task(n: int) -> str:
    """A sample task that takes time to execute."""
    time.sleep(1)
    return f"Task {n+1} completed"

# Using ThreadPoolExecutor for thread management
with ThreadPoolExecutor(max_workers=2) as executor:
    futures = [executor.submit(task, i) for i in range(10)]
    for future in futures:
        print(future.result())

# Data fetching using Multi-Threading

import requests
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict
import pandas as pd
from IPython.display import display

def fetch_url(url: str) -> Dict[str, str]:
    """Fetches the status code and response time for a URL."""
    start_time = time.time()
    response = requests.get(url, timeout=5)
    end_time = time.time()
    return {
        "URL": url,
        "Status Code": response.status_code,
        "Response Time (s)": round(end_time - start_time, 2)
    }

def threaded_scraper(urls: List[str]) -> pd.DataFrame:
    """Fetches multiple URLs concurrently and returns results as a DataFrame."""
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(fetch_url, urls))
    return pd.DataFrame(results)

urls_list = [
    "https://www.amrita.edu/campus/amritapuri/",
    "https://www.github.com",
    "https://www.stackoverflow.com",
    "https://www.reddit.com",
    "https://www.wikipedia.org"
]

df_results = threaded_scraper(urls_list)
display(df_results)

# I/O bound Stock Price Fetcher

import random
from typing import List


def fetch_stock_price(stock: str) -> None:
    """Fetches a simulated stock price."""
    while True:
        price = round(random.uniform(100, 500), 2)
        print(f"{stock}: ${price}")
        time.sleep(1)


def main() -> None:
    """Fetches multiple stock prices concurrently using threads."""
    stocks: List[str] = ["AAPL", "GOOG", "AMZN", "MSFT", "TSLA"]

    threads: List[threading.Thread] = []
    for stock in stocks:
        thread = threading.Thread(target=fetch_stock_price, args=(stock,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()


if __name__ == "__main__":
    main()

"""# Multi-Processing"""

# Simple Process

import multiprocessing

def worker():
    print("Worker process started")
    time.sleep(2)
    print("Worker process finished")

# Process Creation
process = multiprocessing.Process(target=worker)
process.start()

# Main process execution
print("Main process running...")

# Waiting for the process to finish
process.join()
print("Worker process completed.")

# Multiple Processes running

def worker(task_id: int) -> None:
    """Function executed in a separate process."""
    print(f"Process {task_id} started")
    time.sleep(1)
    print(f"Process {task_id} completed")

# Creating multiple processes
processes = []
for i in range(5):
    process = multiprocessing.Process(target=worker, args=(i,))
    processes.append(process)
    process.start()

# Waiting for all processes to complete
for process in processes:
    process.join()

print("All processes completed.")

# Parallel Computation

from multiprocessing import Array, Value, Lock
from typing import List


def square_list(mylist: List[int], result: Array, square_sum: Value, lock: Lock) -> None:
    """
    Computes the square of each element in the given list and stores the results
    in a shared memory array. Also calculates and stores the sum of squares.
    """
    with lock:
        for idx, num in enumerate(mylist):
            result[idx] = num * num


        square_sum.value = sum(result)

    print(f"Result (in process p1): {list(result)}")
    print(f"Sum of squares (in process p1): {square_sum.value}")


if __name__ == "__main__":

    mylist: List[int] = [1, 2, 3, 4, 5, 6]

    result: Array = multiprocessing.Array('i', len(mylist))

    square_sum: Value = multiprocessing.Value('i')

    lock = multiprocessing.Lock()

    p1 = multiprocessing.Process(target=square_list, args=(mylist, result, square_sum, lock))
    p1.start()

    p1.join()

    print(f"Result (in main program): {list(result)}")
    print(f"Sum of squares (in main program): {square_sum.value}")

# Pipes

"""
Inter-Process Communication using Multiprocessing Pipe in Python
"""

import multiprocessing
from multiprocessing.connection import Connection
from typing import List


def sender(conn: Connection, msgs: List[str]) -> None:
    """
    Sends messages through a multiprocessing Pipe.
    """
    for msg in msgs:
        conn.send(msg)
        print(f"Sent the message: {msg}")
    conn.close()


def receiver(conn: Connection) -> None:
    """
    Receives and prints messages from a multiprocessing Pipe.
    """
    while True:
        msg = conn.recv()
        if msg == "END":
            break
        print(f"Received the message: {msg}")


if __name__ == "__main__":

    msgs: List[str] = ["hello", "hey", "hru?", "END"]

    parent_conn, child_conn = multiprocessing.Pipe()

    p1 = multiprocessing.Process(target=sender, args=(parent_conn, msgs))
    p2 = multiprocessing.Process(target=receiver, args=(child_conn,))

    p1.start()
    p2.start()

    p1.join()
    p2.join()

# Using ProcessPoolExecutor
from concurrent.futures import ProcessPoolExecutor
import time
import os

def compute(n: int) -> int:
    """Simulate a CPU-intensive task."""
    try:
        time.sleep(1)
        return n * n
    except Exception as e:
        print(f"Error in process {os.getpid()}: {e}")
        return -1

if __name__ == "__main__":
    with ProcessPoolExecutor(max_workers=3) as executor:
        results = executor.map(compute, range(5))

    for result in results:
        print(f"Computed result: {result}")

# Large Dataset processing using Multi-Processing

import multiprocessing
import numpy as np
from typing import List
import pandas as pd
from IPython.display import display

def compute_square(numbers: List[int]) -> List[int]:
    """Computes the square of each number."""
    return [n ** 2 for n in numbers]

data = np.random.randint(1, 1000, size=(1000000,)).tolist()
num_chunks = 4
chunk_size = len(data) // num_chunks
chunks = [data[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks)]

with multiprocessing.Pool(processes=num_chunks) as pool:
    results = pool.map(compute_square, chunks)

final_result = [item for sublist in results for item in sublist]  # Flatten the list

df_numbers = pd.DataFrame({"Original": data[:100], "Squared": final_result[:100]})  # Display sample

# Display DataFrame
display(df_numbers)

# Parallel Matrix Multiplication using MultiProcessing

import numpy as np
from typing import Tuple


def multiply_matrices(matrix_pair: Tuple[np.ndarray, np.ndarray]) -> np.ndarray:
    """Multiplies two matrices and returns the result."""
    result = np.dot(matrix_pair[0], matrix_pair[1])
    return result


def main() -> None:
    """Performs matrix multiplications in parallel using multiprocessing."""
    matrix_size = 500
    matrices = [(np.random.rand(matrix_size, matrix_size), np.random.rand(matrix_size, matrix_size)) for _ in range(4)]

    with multiprocessing.Pool(processes=4) as pool:
        results = pool.map(multiply_matrices, matrices)
    print(matrices)
    print("Matrix multiplications completed!")


if __name__ == "__main__":
    main()

"""# Async Processing"""

# Running a async function

import asyncio

async def say_hello():
    await asyncio.sleep(1)
    print("Hello, Async!")

asyncio.run(say_hello())

# Running Multiple Coroutines

async def task(n: int) -> str:
    """Simulate an async task."""
    await asyncio.sleep(1)
    return f"Task {n} completed"

async def main():
    tasks = [task(i) for i in range(5)]
    results = await asyncio.gather(*tasks)
    for result in results:
        print(result)

asyncio.run(main())

# Using asyncio with HTTP Requests
import asyncio
import aiohttp

async def fetch(url: str) -> str:
    """Fetch data from a URL asynchronously."""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

async def main():
    urls = [
        "https://www.google.com",
        "https://www.python.org",
        "https://www.github.com"
    ]
    tasks = [fetch(url) for url in urls]
    responses = await asyncio.gather(*tasks)

    for url, response in zip(urls, responses):
        print(f"Fetched {len(response)} characters from {url}")

asyncio.run(main())

# Async Web Scraper with AIOHTTP
from typing import List, Dict
import nest_asyncio

nest_asyncio.apply()

async def fetch(url: str, session: aiohttp.ClientSession) -> Dict[str, str]:
    """Fetches the status code and response time of a URL asynchronously."""
    async with session.get(url) as response:
        return {"URL": url, "Status Code": response.status, "Content Length": len(await response.text())}

async def async_scraper(urls: List[str]) -> pd.DataFrame:
    """Scrapes multiple URLs asynchronously using aiohttp."""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(url, session) for url in urls]
        responses = await asyncio.gather(*tasks)
        return pd.DataFrame(responses)

urls_list = [
    "https://www.python.org",
    "https://www.github.com",
    "https://www.stackoverflow.com",
    "https://www.reddit.com",
    "https://www.wikipedia.org"
]

df_async_results = await async_scraper(urls_list)

# Display DataFrame
df_async_results